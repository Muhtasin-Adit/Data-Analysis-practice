{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Age        Salary\n",
      "count  11.000000     12.000000\n",
      "mean   35.363636  62166.666667\n",
      "std     6.888726  14837.503670\n",
      "min    27.000000  38000.000000\n",
      "25%    30.500000  51000.000000\n",
      "50%    35.000000  61000.000000\n",
      "75%    39.000000  73750.000000\n",
      "max    48.000000  85000.000000\n"
     ]
    }
   ],
   "source": [
    "print(d1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dependent and independent vairable vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing the last boolean col from the dataframe with -1 command and saving as x.\n",
    "Here, x is the independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d1.iloc[:,:-1].values       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the previous section had the : before -1 which which indicated slicing, but here only the -1 is used depicting only the last bool col is taken and saved in y.\n",
    "Here, y is the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = d1.iloc[:,-1].values       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mumbai' 27.0 51000.0]\n",
      " ['New York' 27.0 48000.0]\n",
      " ['Mumbai' 30.0 52000.0]\n",
      " ['New York' nan 66000.0]\n",
      " ['Tokyo' 48.0 nan]\n",
      " ['Tokyo' nan 51000.0]\n",
      " ['Singapore' 33.0 69000.0]\n",
      " ['New York' 40.0 79000.0]\n",
      " ['Mumbai' 38.0 nan]\n",
      " ['Singapore' 35.0 38000.0]\n",
      " ['Tokyo' nan 56000.0]\n",
      " ['Singapore' 35.0 72000.0]\n",
      " ['New York' 45.0 79000.0]\n",
      " ['Mumbai' 31.0 85000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes' 'Yes' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes'\n",
      " 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City                     0\n",
      "Age                      3\n",
      "Salary                   2\n",
      "Eligibility for bonus    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(d1.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice of using dropna.\n",
    "It's not gonna be used in the main process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're dropping only the rows that are on main dataframe rather than the splitted dataframe which are x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.dropna(inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         City   Age   Salary Eligibility for bonus\n",
      "0      Mumbai  27.0  51000.0                   Yes\n",
      "1    New York  27.0  48000.0                   Yes\n",
      "2      Mumbai  30.0  52000.0                    No\n",
      "6   Singapore  33.0  69000.0                    No\n",
      "7    New York  40.0  79000.0                   Yes\n",
      "9   Singapore  35.0  38000.0                    No\n",
      "11  Singapore  35.0  72000.0                    No\n",
      "12   New York  45.0  79000.0                   Yes\n",
      "13     Mumbai  31.0  85000.0                   Yes\n"
     ]
    }
   ],
   "source": [
    "print(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is used for handling missing values, we will use it in splitted dataframes (x,y).\n",
    "Here, the strategy value is usually used 'mean', but we can also use std, min, max etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mumbai' 27.0 51000.0]\n",
      " ['New York' 27.0 48000.0]\n",
      " ['Mumbai' 30.0 52000.0]\n",
      " ['New York' nan 66000.0]\n",
      " ['Tokyo' 48.0 nan]\n",
      " ['Tokyo' nan 51000.0]\n",
      " ['Singapore' 33.0 69000.0]\n",
      " ['New York' 40.0 79000.0]\n",
      " ['Mumbai' 38.0 nan]\n",
      " ['Singapore' 35.0 38000.0]\n",
      " ['Tokyo' nan 56000.0]\n",
      " ['Singapore' 35.0 72000.0]\n",
      " ['New York' 45.0 79000.0]\n",
      " ['Mumbai' 31.0 85000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the imputer function, we are insertig the cols starting after 1st column(0, 1, 2, 3) till the 3rd col. In the 0th index, the index number of the data is stored.\n",
    "After fitting the values in the impter function, we replace the new value from imputer function \n",
    "with the old value in x using transform function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(x[:, 1:3])\n",
    "x[:,1:3] = imputer.transform(x[:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mumbai' 27.0 51000.0]\n",
      " ['New York' 27.0 48000.0]\n",
      " ['Mumbai' 30.0 52000.0]\n",
      " ['New York' 35.36363636363637 66000.0]\n",
      " ['Tokyo' 48.0 62166.666666666664]\n",
      " ['Tokyo' 35.36363636363637 51000.0]\n",
      " ['Singapore' 33.0 69000.0]\n",
      " ['New York' 40.0 79000.0]\n",
      " ['Mumbai' 38.0 62166.666666666664]\n",
      " ['Singapore' 35.0 38000.0]\n",
      " ['Tokyo' 35.36363636363637 56000.0]\n",
      " ['Singapore' 35.0 72000.0]\n",
      " ['New York' 45.0 79000.0]\n",
      " ['Mumbai' 31.0 85000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing OneHotEncoder to use binary values instead of the string values from col 1 which is the name of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ColumnTransformer function we can transform the string data into binary data.\n",
    "In this method the must use 2 attributes are transformers and remainders.\n",
    "In the transformers attribute, we have to define it is a encoder which with the value of OneHotEncoder, and then the index of the desired col in the dataframe x.\n",
    "Later, the remainder attribute is defined. The default value of the remainder attribute is 'drop' which means if we use drop, the desired column will be transformed into binary data and the remaining columns will be dropped from the dataframe x which we certainly do not want. \n",
    "So, we'll use the value 'passthrough' which means that the remaining columns in the dataframe \n",
    "x will be left as they were.\n",
    "Finnaly, we'll use np.array to fit the data and replace with the old data in x.\n",
    "'n' number of new columns will be created in dataframe x, 'n' is the number of unique values \n",
    "from the desired index that we passed in the transformers attribute.\n",
    "In the final form, each row will depict a value of '1' which means 'True' in the respected row that is indicating the column associated with the unique value from the original index of name of the cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(),[0])], remainder='passthrough')\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 0.0 27.0 51000.0]\n",
      " [0.0 1.0 0.0 0.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 0.0 30.0 52000.0]\n",
      " [0.0 1.0 0.0 0.0 35.36363636363637 66000.0]\n",
      " [0.0 0.0 0.0 1.0 48.0 62166.666666666664]\n",
      " [0.0 0.0 0.0 1.0 35.36363636363637 51000.0]\n",
      " [0.0 0.0 1.0 0.0 33.0 69000.0]\n",
      " [0.0 1.0 0.0 0.0 40.0 79000.0]\n",
      " [1.0 0.0 0.0 0.0 38.0 62166.666666666664]\n",
      " [0.0 0.0 1.0 0.0 35.0 38000.0]\n",
      " [0.0 0.0 0.0 1.0 35.36363636363637 56000.0]\n",
      " [0.0 0.0 1.0 0.0 35.0 72000.0]\n",
      " [0.0 1.0 0.0 0.0 45.0 79000.0]\n",
      " [1.0 0.0 0.0 0.0 31.0 85000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing LabelEncoder to transform the 2 possible values in the bool col in the dependent variable that we seperated and saved as y.\n",
    "In this case, there could be 2 possible values which are 'True' and 'False'. So, we'll replace these 2 values with the labels 0,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes' 'Yes' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes'\n",
      " 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable le is used to instantiate the constructor LabelEncoder(). \n",
    "We then used the fit_transform function to fit the data in the LabelEncoder and replaced the new data with the old data in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 1 0 0 1 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dataset into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 0.0 30.0 52000.0]\n",
      " [0.0 0.0 0.0 1.0 35.36363636363637 56000.0]\n",
      " [0.0 0.0 0.0 1.0 48.0 62166.666666666664]\n",
      " [0.0 1.0 0.0 0.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 0.0 45.0 79000.0]\n",
      " [1.0 0.0 0.0 0.0 27.0 51000.0]\n",
      " [1.0 0.0 0.0 0.0 31.0 85000.0]\n",
      " [0.0 0.0 1.0 0.0 35.0 38000.0]\n",
      " [1.0 0.0 0.0 0.0 38.0 62166.666666666664]\n",
      " [0.0 0.0 1.0 0.0 35.0 72000.0]\n",
      " [0.0 0.0 0.0 1.0 35.36363636363637 51000.0]]\n",
      "[[0.0 1.0 0.0 0.0 35.36363636363637 66000.0]\n",
      " [0.0 1.0 0.0 0.0 40.0 79000.0]\n",
      " [0.0 0.0 1.0 0.0 33.0 69000.0]]\n",
      "[0 0 1 1 1 1 1 0 1 0 0]\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the data we've got, we can see that the scale the values are in, they're not same or close to be fair enough. So, to get the best results we need to put all the data we have into the same scale and that is what feature scaling is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're 2 forms of feature scaling, which are standardization and normalization.\n",
    "But when do we use which one? Standardization works on any kind of data where we can use the normalization method only when the data is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, in this case, we are going to use standardization.\n",
    "Usually, the scale of standardization is between -3 and +3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 0.0 -0.808993598746295 -0.5674179192969602]\n",
      " [0.0 0.0 0.0 1.0 0.032411602513874406 -0.27137378748985014]\n",
      " [0.0 0.0 0.0 1.0 2.0147052122624074 0.1850275823794443]\n",
      " [0.0 1.0 0.0 0.0 -1.2796100672477455 -0.8634620511040701]\n",
      " [0.0 1.0 0.0 0.0 1.544088743760957 1.4308799704010324]\n",
      " [1.0 0.0 0.0 0.0 -1.2796100672477455 -0.6414289522487376]\n",
      " [1.0 0.0 0.0 0.0 -0.6521214425791448 1.8749461681116975]\n",
      " [0.0 0.0 1.0 0.0 -0.02463281791054433 -1.6035723806218452]\n",
      " [1.0 0.0 0.0 0.0 0.44598365059090606 0.1850275823794443]\n",
      " [0.0 0.0 1.0 0.0 -0.02463281791054433 0.9128027397385899]\n",
      " [0.0 0.0 0.0 1.0 0.032411602513874406 -0.6414289522487376]]\n",
      "[[0.0 1.0 0.0 0.0 -0.26055806683273924 -0.9596162302465067]\n",
      " [0.0 1.0 0.0 0.0 1.3340573021836235 1.3794483309793555]\n",
      " [0.0 0.0 1.0 0.0 -1.0734992353508865 -0.4198321007328462]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train[:,4:] = scaler.fit_transform(x_train[:,4:])\n",
    "x_test[:,4:] = scaler.fit_transform(x_test[:,4:])\n",
    "print(x_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
